{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "wikipedia.set_lang('de')\n",
    "wikipedia.set_rate_limiting(True, min_wait=datetime.timedelta(0, 0, 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lädt rekursiv alle verlinkten Artikel bis die maximale Tiefe erreicht ist\n",
    "def load_recursive(page, queue, depth=2, with_summary=False, scanned=set()):\n",
    "    if depth <= 0 or (page in scanned and depth == 1):\n",
    "        #print('skipping %s because it already exists and no further depths are being analyzed anyway'%(page))\n",
    "        return scanned\n",
    "    scanned.add(page)\n",
    "\n",
    "    try:\n",
    "        loaded_page = wikipedia.page(page)\n",
    "        if 'Kategorie:Tag' in loaded_page.categories:\n",
    "            print(\"skipped %s\"%(page))\n",
    "            return scanned\n",
    "            \n",
    "        for sub_page in loaded_page.links:\n",
    "            scanned = load_recursive(sub_page, queue, depth - 1, with_summary, scanned)\n",
    "            \n",
    "        content = loaded_page.content\n",
    "        if content:\n",
    "            queue.put({\"depth\":depth, \"page\":page, \"content\":loaded_page.content}, block=True, timeout=None)\n",
    "        if with_summary:\n",
    "            queue.put({\"depth\":depth, \"page\":page, \"content\":loaded_page.summary}, block=True, timeout=None)\n",
    "\n",
    "    except wikipedia.DisambiguationError:\n",
    "        scanned = load_recursive_by_search(page, queue, depth - 1, with_summary, scanned)\n",
    "    except wikipedia.PageError:\n",
    "        return scanned\n",
    "\n",
    "    #sys.stdout.write(\"\\r%i pages scanned\" % (len(scanned)))\n",
    "    return scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sucht nach dem übergebenen Begriff und führt die rekursive auf jedes der Ergebnisse aus\n",
    "def load_recursive_by_search(search, queue, depth=1, with_summary=False, scanned = set()):\n",
    "    search_result = wikipedia.search(search)\n",
    "    for result in search_result:\n",
    "        scanned = load_recursive(result, queue, depth, with_summary, scanned)\n",
    "    return scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_process(search_list, queue, depth, with_summary):\n",
    "    scanned = set(os.listdir('collected_articles/'))\n",
    "    #print(\"pages found: %s\"%(scanned))\n",
    "    for search in search_list:\n",
    "        print(\"starting with %s\"%(search))\n",
    "        scanned = load_recursive_by_search(search, queue, depth, with_summary, scanned)\n",
    "        print(\"%i sites have been scanned\"%(len(scanned)))    \n",
    "    queue.put(None, block=True, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_async_scraping(search_list, max_queue_size = 100, depth = 3, with_summary=False):\n",
    "    page_queue = Queue(maxsize=max_queue_size)\n",
    "    p = Process(target=search_process, args=(search_list,page_queue,depth,with_summary,))\n",
    "    p.start()\n",
    "    return p, page_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_categories = ['freizeit', \n",
    "           'unterhaltung', \n",
    "           'shopping', \n",
    "           'einkaufen', \n",
    "           'essen', \n",
    "           'lebensmittel', \n",
    "           'bar', \n",
    "           'restaurant', \n",
    "           'gesundheit', \n",
    "           'drogerie', \n",
    "           'abonnement', \n",
    "           'spende', \n",
    "           'berufsausgaben',\n",
    "           'einzelhändler',\n",
    "           'bildung', \n",
    "           'familie', \n",
    "           'freunde', \n",
    "           'gehalt', \n",
    "           'haushalt', \n",
    "           'nebenkosten', \n",
    "           'medien', \n",
    "           'elektronik',\n",
    "           'reisen',\n",
    "           'urlaub',            \n",
    "           'sparen', \n",
    "           'investieren', \n",
    "           'steuern', \n",
    "           'abgaben', \n",
    "           'transport',\n",
    "           'auto', \n",
    "           'versicherungen',\n",
    "           'finanzen'\n",
    "          ]\n",
    "categories = [\n",
    "           'bildung',\n",
    "           'abonnement',\n",
    "           'elektronik',\n",
    "           'reisen',\n",
    "           'urlaub',\n",
    "           'versicherungen',\n",
    "           'einzelhändler',\n",
    "           'Ernährung des Menschen',\n",
    "           'Nahrung',\n",
    "           'Gericht (Speise)',\n",
    "           'lebensmittel', \n",
    "           'bar', \n",
    "           'restaurant', \n",
    "           'gesundheit', \n",
    "           'drogerie',\n",
    "           'spende', \n",
    "           'berufsausgaben',\n",
    "           'familie', \n",
    "           'freunde', \n",
    "           'gehalt', \n",
    "           'haushalt', \n",
    "           'nebenkosten', \n",
    "           'medien',             \n",
    "           'sparen', \n",
    "           'investieren', \n",
    "           'steuern', \n",
    "           'abgaben', \n",
    "           'transport',\n",
    "           'auto', \n",
    "           'finanzen'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process, queue = start_async_scraping(categories)\n",
    "for document in iter(queue.get, None):\n",
    "    print(\"depth %i loaded page %s with length %i\"%(document['depth'], document['page'], len(document['content'])))\n",
    "    f = open('collected_articles/'+document['page'].replace(\"/\",\"\"), \"w\")\n",
    "    f.write(document['content'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
